{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Working with Pipelines Through REST API\n",
    "\n",
    "- **Level**: Advanced\n",
    "- **Time to complete**: *Enter the time it takes to complete this tutorial, in minutes.*\n",
    "- **Prerequisites**: \n",
    "    - Create a folder where you'll store the files for this project.\n",
    "    - Create a fresh virtual environment.\n",
    "    - A Docker instance installed and ready.\n",
    "- **Components**: REST API, indexing Pipeline, RAG Pipeline\n",
    "- **Goal**: After completing this tutorial, you will have a RAG system that you can query through REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial shows how to create custom REST API endpoints for uploading and indexing files, and then querying your Pipeline. It uses an indexing and a simple RAG Pipeline. \n",
    "\n",
    "> You can also have a look at our example [Haystack RAG application](https://github.com/deepset-ai/haystack-rest-api) on GitHub as a practical example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Haystack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install haystack-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Your Document Store\n",
    "For this tutorial, use the Elasticsearch Document Store as it allows both keyword-based and vector-based retrieval.\n",
    "First, install `ElasticsearchDocumentStore`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install elasticsearch-haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use a local Elasticsearch instance through Docker:\n",
    "1. Start Elasticsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -d -p 9200:9200 -e \"discovery.type=single-node\" -e \"ES_JAVA_OPTS=-Xms1024m -Xmx1024m\" -e \"xpack.security.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Verify it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl --fail http://localhost:9200/_cat/health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything's fine, you should get a response similar to this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1697807598 13:13:18 docker-cluster green 1 1 0 0 0 0 0 0 - 100.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use Elastic Cloud:\n",
    "1. Start a free trial of Elastic Cloud and [create a deployment](https://dev.to/lisahjung/part-3-create-an-elastic-cloud-deployment-36bn).\n",
    "2. [Connect to Elastic Cloud](https://elasticsearch-py.readthedocs.io/en/v8.10.1/quickstart.html#connecting).\n",
    "\n",
    "Your Document Store is now ready. You can now move on to Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Your Pipelines\n",
    "\n",
    "For deployment, it's best to use serialized versions of your Pipelines.\n",
    "You can use the [example YAML Pipelines](https://github.com/deepset-ai/haystack-rest-api/tree/main/src/pipelines) in this tutorial or serialize your own Python Pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"my_pipeline.yaml\", \"w\") as f:\n",
    "    my_python_pipeline.dump(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example Pipelines work with a local instance of Elasticsearch. To use Elastic Cloud, change the `init_parameters` of the Document Store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Cloud\n",
    "document_store:\n",
    "        init_parameters:\n",
    "          cloud_id: YOUR-CLOUD-ID\n",
    "          api_key: YOUR-API-KEY\n",
    "\t\t\t\t\tindex: default\n",
    "        type: ElasticsearchDocumentStore\n",
    "\n",
    "# local Elasticsearch instance\n",
    "document_store:\n",
    "        init_parameters:\n",
    "          hosts: http://localhost:9200\n",
    "          index: default\n",
    "        type: ElasticsearchDocumentStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Your Local REST API\n",
    "Time to create the endpoints. You will use a Python framework, [FastAPI](https://fastapi.tiangolo.com/), to build the API. Then, use [Uvicorn](https://www.uvicorn.org/) to serve it. \n",
    "\n",
    "1. Install FastAPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fastapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Install Uvicorn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"uvicorn[standard]‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Status Endpoint\n",
    "Create a file *scr/main.py* with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.get(\"/ready\")\n",
    "def check_status():\n",
    "\t\t\"\"\"\n",
    "    Use this endpoint during startup to check if the\n",
    "    server is ready to take requests.\n",
    "\n",
    "    The recommended approach is to call this endpoint with a short timeout,\n",
    "    like 500ms, and if there's no reply, it means the server is busy.\n",
    "    \"\"\"\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a simple API endpoint that returns `True` if the REST API finished loading and is ready to accept requests.\n",
    "\n",
    "Now, run the REST API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uvicorn src.main:app --host 0.0.0.0 --port 8000  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try submitting requests to the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X 'GET' \\\n",
    "'http://127.0.0.1:8000/ready' \\\n",
    "-H 'accept: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `True` as the response.\n",
    "To view the API documentation, go to http://localhost:8000/docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Upload/Indexing Endpoint\n",
    "Create an endpoint to allow uploading and indexing files to the Elasticsearch Document Store. The endpoint should temporarily save the uploaded files, run the indexing Pipeline on them, and return the result.\n",
    "\n",
    "Add the following code to the *src/main.py* file to create the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "\n",
    "from haystack.preview import Pipeline\n",
    "\n",
    "from haystack.preview.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.preview.components.file_converters import TextFileToDocument\n",
    "from haystack.preview.components.writers import DocumentWriter\n",
    "from elasticsearch_haystack.document_store import ElasticsearchDocumentStore\n",
    "from elasticsearch_haystack.bm25_retriever import ElasticsearchBM25Retriever\n",
    "\n",
    "# specify the path to your indexing Pipeline\n",
    "app = FastAPI()\n",
    "with open(\"./src/pipelines/indexing_pipeline.yaml\", \"rb\") as f:\n",
    "    indexing_pipeline = Pipeline.load(f)\n",
    "\n",
    "FILE_UPLOAD_PATH = os.getenv(\"FILE_UPLOAD_PATH\", str((Path(__file__).parent.parent / \"file-upload\").absolute()))\n",
    "Path(FILE_UPLOAD_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "@app.post(\"/file-upload\")\n",
    "def upload_files(\n",
    "    files: List[UploadFile] = File(...), keep_files: Optional[bool] = False):\n",
    "    \"\"\"\n",
    "    Upload a list of files to be indexed.\n",
    "\n",
    "    Note: Files are removed immediately after being indexed. To keep them, pass the\n",
    "    `keep_files=true` parameter in the request payload.\n",
    "    \"\"\"\n",
    "\n",
    "    file_paths: list = []\n",
    "\n",
    "    for file_to_upload in files:\n",
    "        try:\n",
    "            file_path = Path(FILE_UPLOAD_PATH) / f\"{uuid.uuid4().hex}_{file_to_upload.filename}\"\n",
    "            with file_path.open(\"wb\") as fo:\n",
    "                fo.write(file_to_upload.file.read())\n",
    "            file_paths.append(file_path)\n",
    "        finally:\n",
    "            file_to_upload.file.close()\n",
    "\n",
    "    result=indexing_pipeline.run({\"converter\": {\"sources\": file_paths}})\n",
    "\n",
    "    # Clean up indexed files\n",
    "    if not keep_files:\n",
    "        for p in file_paths:\n",
    "            p.unlink()        \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the endpoint, upload the Austalia Wikipedia page to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget -O australia.txt \"https://en.wikipedia.org/w/index.php?title=Australia&action=raw&ctype=text\"\n",
    "\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost:8000/file-upload?keep_files=false' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: multipart/form-data' \\\n",
    "  -F 'files=@australia.txt;type=text/plain'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tip: You can also use your local API docs to do this*.\n",
    "You should get a response similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"writer\": {\n",
    "    \"documents_written\": 20\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Your processed Documents are now in the Document Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Query Endpoint\n",
    "\n",
    "Now, create an endpoint to query our RAG Pipeline. Add the following code to the *src/main.py* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "from haystack.preview import Pipeline\n",
    "\n",
    "from haystack.preview.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.preview.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.preview.components.generators import GPTGenerator\n",
    "from elasticsearch_haystack.document_store import ElasticsearchDocumentStore\n",
    "from elasticsearch_haystack.bm25_retriever import ElasticsearchBM25Retriever\n",
    "\n",
    "# this is the path to the RAG Pipeline\n",
    "app = FastAPI()\n",
    "with open(\"./src/pipelines/rag_pipeline.yaml\", \"rb\") as f:\n",
    "    rag_pipeline = Pipeline.load(f)\n",
    "\n",
    "@app.get(\"/query\")\n",
    "def ask_rag_pipeline(query:str):\n",
    "    \"\"\"\n",
    "    Ask a question to the RAG Pipeline.\n",
    "    \"\"\"\n",
    "    result = rag_pipeline.run({\n",
    "        \"retriever\": {\"query\": query}, \n",
    "        \"prompt_builder\": {\"question\": query}, \n",
    "        \"answer_builder\": {\"query\": query}\n",
    "    })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the endpoint. You can try questions like \"who are Torrest Strait Islanders?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X 'GET' \\\n",
    "  'http://127.0.0.1:8000/query?query=Who%20are%20Torres%20Strait%20Islanders%3F' \\\n",
    "  -H 'accept: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a response similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"answer_builder\": {\n",
    "    \"answers\": [\n",
    "      {\n",
    "        \"data\": \"Torres Strait Islanders are ethnically Melanesian people who obtained their livelihood from seasonal horticulture and the resources of their reefs and seas.\",\n",
    "        \"query\": \"Who are Torres Strait Islanders?\",\n",
    "        \"metadata\": {\n",
    "          \"model\": \"gpt-3.5-turbo-0613\",\n",
    "          \"index\": 0,\n",
    "          \"finish_reason\": \"stop\",\n",
    "          \"usage\": {\n",
    "            \"prompt_tokens\": 727,\n",
    "            \"completion_tokens\": 29,\n",
    "            \"total_tokens\": 756\n",
    "          }\n",
    "        },\n",
    "        \"documents\": [...]\n",
    "\t\t\t}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've created three endpoints for your REST API.\n",
    "\n",
    "<details>\n",
    "<summary>Click here to see what the complete file should look like.</summary>\n",
    "\n",
    "```\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from haystack.preview import Pipeline\n",
    "\n",
    "# Needed to load the Pipeline without errors (https://github.com/deepset-ai/haystack/issues/6186)\n",
    "from haystack.preview.components.preprocessors import (\n",
    "    DocumentCleaner,\n",
    "    DocumentSplitter,\n",
    ")\n",
    "from haystack.preview.components.file_converters import TextFileToDocument\n",
    "from haystack.preview.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.preview.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.preview.components.generators import GPTGenerator\n",
    "from haystack.preview.components.writers import DocumentWriter\n",
    "from elasticsearch_haystack.document_store import ElasticsearchDocumentStore\n",
    "from elasticsearch_haystack.bm25_retriever import ElasticsearchBM25Retriever\n",
    "\n",
    "app = FastAPI(title=\"My Haystack RAG API\")\n",
    "\n",
    "# Load the Pipelines from the YAML files\n",
    "with open(\"./src/pipelines/indexing_pipeline.yaml\", \"rb\") as f:\n",
    "    indexing_pipeline = Pipeline.load(f)\n",
    "with open(\"./src/pipelines/rag_pipeline.yaml\", \"rb\") as f:\n",
    "    rag_pipeline = Pipeline.load(f)\n",
    "\n",
    "# Create the file upload directory if it doesn't exist\n",
    "FILE_UPLOAD_PATH = os.getenv(\n",
    "    \"FILE_UPLOAD_PATH\", str((Path(__file__).parent.parent / \"file-upload\").absolute())\n",
    ")\n",
    "Path(FILE_UPLOAD_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "@app.get(\"/ready\")\n",
    "def check_status():\n",
    "    \"\"\"\n",
    "    Use this endpoint during startup to check if the\n",
    "    server is ready to take requests.\n",
    "\n",
    "    The recommended approach is to call this endpoint with a short timeout,\n",
    "    like 500ms, and if there's no reply, it means the server is busy.\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "\n",
    "@app.post(\"/file-upload\")\n",
    "def upload_files(\n",
    "    files: List[UploadFile] = File(...), keep_files: Optional[bool] = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload a list of files to be indexed.\n",
    "\n",
    "    Note: files are removed immediately after being indexed. If you want to keep them, pass the\n",
    "    `keep_files=true` parameter in the request payload.\n",
    "    \"\"\"\n",
    "\n",
    "    file_paths: list = []\n",
    "\n",
    "    for file_to_upload in files:\n",
    "        try:\n",
    "            file_path = (\n",
    "                Path(FILE_UPLOAD_PATH) / f\"{uuid.uuid4().hex}_{file_to_upload.filename}\"\n",
    "            )\n",
    "            with file_path.open(\"wb\") as fo:\n",
    "                fo.write(file_to_upload.file.read())\n",
    "            file_paths.append(file_path)\n",
    "        finally:\n",
    "            file_to_upload.file.close()\n",
    "\n",
    "    result = indexing_pipeline.run({\"converter\": {\"paths\": file_paths}})\n",
    "\n",
    "    # Clean up indexed files\n",
    "    if not keep_files:\n",
    "        for p in file_paths:\n",
    "            p.unlink()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "@app.get(\"/query\")\n",
    "def ask_rag_pipeline(query: str):\n",
    "    \"\"\"\n",
    "    Ask a question to the RAG Pipeline.\n",
    "    \"\"\"\n",
    "    result = rag_pipeline.run(\n",
    "        {\n",
    "            \"retriever\": {\"query\": query},\n",
    "            \"prompt_builder\": {\"question\": query},\n",
    "            \"answer_builder\": {\"query\": query},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return result\n",
    "    ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Your App\n",
    "Time to deploy your app to the server with Docker. If you need more information on how to use FastAPI in containers, see [FastAPI documentation](https://fastapi.tiangolo.com/deployment/docker/).\n",
    "\n",
    "### Create a Docker Image for Your REST API\n",
    "\n",
    "1. Add the package requirements for your app. Create a *requirements.txt* file in the root folder of your project. Here's what the file may look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haystack\n",
    "haystack-ai==2.0.0a\n",
    "\n",
    "# Elasticsearch Document Store\n",
    "elasticsearch-haystack==0.0.2\n",
    "\n",
    "# REST API\n",
    "fastapi==0.104.0\n",
    "uvicorn[standard]==0.23.2\n",
    "python-multipart==0.0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create the Dockerfile with instructions for building the Docker image of your app. For more details, see [FastAPI documentation](https://fastapi.tiangolo.com/deployment/docker/#dockerfile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM python:3.10-slim\n",
    "\n",
    "# installing git is only necessary because elasticsearch-haystack is not yet a package\n",
    "RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "WORKDIR /code\n",
    "\n",
    "COPY ./requirements.txt /code/requirements.txt\n",
    "\n",
    "RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\n",
    "\n",
    "COPY ./src /code/src\n",
    "\n",
    "CMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create the Docker image of your app. To create the Docker image of an app called *my-haystack-app*, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker build -t my-haystack-app ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, everything's in place for you to run the Docker container of your app locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Docker Container of Your App\n",
    "\n",
    "#### Using a Local Elasticsearch Instance\n",
    "\n",
    "First, make sure the Elasticsearch container is started. Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -d -p 9200:9200 -e \"discovery.type=single-node\" -e \"ES_JAVA_OPTS=-Xms1024m -Xmx1024m\" -e \"xpack.security.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Elasticsearch and your app must share the same network, as a workaround, run the container in a [host network mode](https://docs.docker.com/network/drivers/host/) (`--network=\"host\"`).\n",
    "\n",
    "*Tip:* If you want to experiment locally, you can also replace the `ElasticsearchDocumentStore` with an ephemeral `InMemoryDocumentStore`. \n",
    "\n",
    "Run the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run --network=\"host\" -e OPENAI_API_KEY=${OPENAI_API_KEY} my-haystack-app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this exports the local environment variable together with the OpenAI API key inside the container.\n",
    "\n",
    "#### Using Elastic Cloud\n",
    "\n",
    "Run the Docker container with the following command. Note that it exports the local environment variable together with the OpenAI API key inside the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -p 8000:8000 -e OPENAI_API_KEY=${OPENAI_API_KEY} my-haystack-app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Docker Compose\n",
    "\n",
    "Docker Compose is a tool that helps you define and share multi-container applications. With Compose, you can create a YAML file to define the services, and with a single command, you can spin everything up or tear it all down.\n",
    "\n",
    "You can find more information about it in Docker Compose [documentation](https://docs.docker.com/get-started/08_using_compose).\n",
    "\n",
    "1. Create a *docker-compose.yml* file in the root folder of your project. If you want to understand more about the syntax of a Docker Compose file, you can read through the Docker [reference](https://docs.docker.com/compose/compose-file/compose-file-v3/). Here's what the YAML files may look like:\n",
    "\n",
    "#### Using a Local Elasticsearch Instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "version: \"3\"\n",
    "\n",
    "services:\n",
    "  elasticsearch:\n",
    "    image: \"docker.elastic.co/elasticsearch/elasticsearch:8.11.1\"\n",
    "    ports:\n",
    "      - 9200:9200\n",
    "    restart: on-failure\n",
    "    environment:\n",
    "      - discovery.type=single-node\n",
    "      - xpack.security.enabled=false\n",
    "      - \"ES_JAVA_OPTS=-Xms1024m -Xmx1024m\"\n",
    "    healthcheck:\n",
    "        test: curl --fail http://localhost:9200/_cat/health || exit 1\n",
    "        interval: 10s\n",
    "        timeout: 1s\n",
    "        retries: 10\n",
    "\n",
    "  rest-api:\n",
    "    build:\n",
    "      context: .\n",
    "    ports:\n",
    "      - 8000:8000\n",
    "    restart: on-failure\n",
    "    environment:\n",
    "      - OPENAI_API_KEY=${OPENAI_API_KEY}\n",
    "    depends_on:\n",
    "      elasticsearch:\n",
    "        condition: service_healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to expand the detailed explanation of this YAML file.</summary>\n",
    "\n",
    "You can see two services defined in this file: one for Elasticsearch and one for the REST API.\n",
    "\n",
    "The Elasticsearch service:\n",
    "- Uses the official Elasticsearch Docker image.\n",
    "- Follows the convention HOST_PORT:CONTAINER_PORT. Maps TCP port 9200 in the container to port 9200 on the Docker host. You can change the first port according on your needs.\n",
    "- Has some environment variables for configuration. You can find more information about them in the [Elasticsearch documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#docker-cli-run-dev-mode).\n",
    "- Has a `healthcheck` based on `curl``: checks whether the container has been started correctly and is in a healthy state.\n",
    "\n",
    "The REST API service:\n",
    "- You will need to change the Elasticsearch hosts in your YAML Pipelines. When the application ran locally, you used http://localhost:9200. In the default network of your Compose application, the Elasticsearch service is known under the name `elasticsearch`, so the host becomes http://elasticsearch:9200.\n",
    "- `DOCUMENTSTORE_PARAMS_HOST` is the host of the Elasticsearch Document Store. When the application ran locally, you used http://localhost:9200.\n",
    "In the default network of your Compose application, the Elasticsearch service is known under the name elasticsearch, so the host becomes http://elasticsearch:9200.\n",
    "- `context: .` means that the corresponding Docker image will be built based on the Dockerfile located in the root folder of your project (the same folder as the Docker Compose YAML file).\n",
    "- `8000:8000` maps the TCP port 8000 in the container to port 8000 on the Docker host. You can change the first port according on your needs.\n",
    "- The `environment` variable which is needed in our REST API is the `OPENAI_API_KEY` which is needed to call the LLM. In this example, you export your local environment variable with the same name inside the container. You can customize this as you want.\n",
    "- With the `depends_on` condition, you are making sure that the REST API service starts only after the Elasticsearch server has started and is in a healthy state (based on the previously defined `healthcheck`).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Elastic Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "version: \"3\"\n",
    "\n",
    "services:\n",
    "  rest-api:\n",
    "    build:\n",
    "      context: .\n",
    "    ports:\n",
    "      - 8000:8000\n",
    "    restart: on-failure\n",
    "    environment:\n",
    "      - OPENAI_API_KEY=${OPENAI_API_KEY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now you can easily spin up your multi-container application with the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker compose up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the interactive API docs on http://localhost:8000/docs, and you can try calling indexing and querying endpoints.\n",
    "\n",
    "To stop the application, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now deployed a Haystack RAG application with REST API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
